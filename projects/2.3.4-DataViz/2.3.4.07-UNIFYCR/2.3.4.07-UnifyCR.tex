\subsubsection{\stid{4.07} UnifyCR -- A file system for burst buffers} 

\paragraph{Overview} 

The view of storage systems for HPC is changing rapidly. Traditional, 
single-target parallel file systems have reached their cost-effective 
scaling limit. As a result, hierarchical storage systems are being designed 
and installed for our nation's next-generation leadership class systems. 
Current designs employ “burst buffers” as a fast cache between compute 
nodes and the parallel file system for data needed by running jobs and 
workflows. Burst buffers are implemented as compute-node local storage 
(e.g., SSD) or as shared intermediate storage (e.g., SSD on shared burst 
buffer nodes).

Because burst buffers present an additional complexity to effectively
using supercomputers, we are developing UnifyCR, a user-level file system, 
highly-specialized for shared file access on HPC systems with distributed 
burst buffers.  UnifyCR will address a major usability 
factor of the CORAL and other future systems, because it will enable 
applications to gain the performance advantages from distributed burst buffers 
while providing ease of use similar to that of a parallel file system.


\paragraph{Key  Challenges}

The hierarchical storage for future HPC systems will include compute-node
local SSDs as burst buffers. This distributed burst buffer design promises
fast, scalable I/O performance because burst buffer bandwidth and capacity
will automatically scale with the compute resources used by jobs and
workflows. However, a major concern for this distributed design is how to
present the disjoint storage devices as a single storage location to
applications that use shared files. The primary issue is that when concurrent
processes on different compute nodes perform I/O operations, e.g., writes,
to a shared file, the data for the file are scattered across the separate
compute-node local burst buffers instead of being stored in a single
location. Consequently, if a process wants to access bytes from the
shared file that exist in the burst buffer of a different compute node,
that process needs to somehow track or look up the information for locating
and retrieving those bytes. Additionally, there is no common interface across
vendors for accessing remote burst buffers, so code for cross-node file
sharing will not be easily portable across multiple DOE systems with
different burst buffer architectures, further increasing programming
complexity to support shared files.

For the reasons outlined above, it is clear that without software support 
for distributed burst buffers, applications
will have major difficulties utilizing these resources. 

\begin{figure}[htb]
        \centering
        \includegraphics[width=3.5in]{projects/2.3.4-DataViz/2.3.4.07-UNIFYCR/UnifyCR-overview}
        \caption{\label{fig:unifycr-overview} \textbf{UnifyCR Overview.} Users will be able to give commands in their batch scripts to launch UnifyCR within
their allocation. UnifyCR will work with POSIX I/O, common I/O libraries, and
VeloC. Once file operations are transparently intercepted by UnifyCR, they
will be handled with specialized optimizations to ensure high performance.}
\end{figure}

\paragraph{Solution Strategy}

To address this concern, we are developing UnifyCR, a user-level file system,
highly-specialized for shared file access on HPC systems with distributed
burst buffers. In Figure \ref{fig:unifycr-overview}, we show a high
level schematic of how UnifyCR will work. Users will load UnifyCR into
their jobs from their batch scripts. Once UnifyCR is instantiated, user
applications can read and write shared files to the mount point just like
they would the parallel file system. File operations to the UnifyCR
mount point will be intercepted and handled with specialized optimizations
that will deliver high I/O performance. 

Because checkpoint/restart has been reported to cause 75-80\%
of the I/O traffic on some HPC systems, we target our approach at
checkpoint/restart workloads. Thus, UnifyCR will address a major usability
factor of the CORAL and other future systems. We will design UnifyCR such
that it transparently intercepts I/O calls, so we expect it to integrate
cleanly with other software including I/O and checkpoint/restart libraries.
Additionally, because UnifyCR is tailored for HPC systems and workloads,
it can deliver high performance.



\paragraph{Recent Progress}

In the second year of this project, the team began
implementing the design plan that was established in the first
year of the project (See Figure \ref{fig:milestone2}). In particular
we worked to replace MPI communication with DataLib software; to integrate
UnifyCR with resource managers; to evaluate and implement support for I/O 
libraries; and to implement support for Sierra and Summit platforms.
We also began to implement a testing framework for continuous integration
testing, detected and fixed bugs, designed and began implementing a 
metadata abstraction layer, and have begun outreach to applications teams.
\begin{figure}[htb]
        \centering
        \includegraphics[width=4in]{projects/2.3.4-DataViz/2.3.4.07-UNIFYCR/milestone2}
        \caption{\label{fig:milestone2} \textbf{UnifyCR Design.} The UnifyCR
instance consists of a dynamic library and a UnifyCR daemon that runs
on each compute node in the job. The library intercepts I/O calls to
the UnifyCR mount point from applications, I/O libraries, or VeloC and communicates them to the UnifyCR daemon that handles the I/O operation.}
\end{figure}


Our source code for UnifyCR is available on 
GitHub at \url{https://github.com/LLNL/UnifyCR}. 
\paragraph{Next Steps}

For our next milestone effort, we are focused on improving the performance
of UnifyCR on Summit and Sierra. Additionally, we are continuing work on 
integrating and supporting ECP collaborator software including:
I/O libraries HDF5, MPI-IO, PnetCDF, and ADIOS; completing the replacement
of MPI communication with Mercury from ECP DataLib; implementing
support for ECP VeloC; and supporting ECP applications such as
E3SM, GEOS, and Chombo.
We also plan to release UnifyCR 1.0 at the end of the fiscal year.

